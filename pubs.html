<html>

<head>
<title>Lihong Li</title>
<meta name="author" content="Lihong Li">
<meta name="description" content="Personal Homepage">
<meta name="keywords" content="Artificial Intelligence, Machine Learning, Reinforcement Learning, Planning, Research, Rutgers, Yahoo!, Microsoft, Google">
<meta name="classification" content="Personal Homepage">
</head>

<body bgcolor="f7f7f7" text="black" link="blue" vlink="purple" alink="red">

<h1> <img src="./img/lihong-pro.jpg"> Lihong Li <img src="./img/chinese-name.jpg" height="25"></h1>

Research Scientist <br/>
Google Inc. <br/><br/>
lihongli.cs@gmail.com (for general academic work) <br/>
lihong@google.com (for Google related work) <br/>
747 Sixth Street South, Kirkland, WA, USA 98033<br/>
<p>

<p/><hr>

[<a href="index.html#interests"><font face="Times New Roman" size="3">Interests</font></a><font face="Times New Roman" size="3">]
  [<a href="index.html#experiences"><font face="Times New Roman" size="3">Experiences</font></a><font face="Times New Roman" size="3">]
[<a href="index.html#activities"><font face="Times New Roman" size="3">Professional Activities</font></a><font face="Times New Roman" size="3">]
[</font><a href="index.html" target="_self"><font face="Times New Roman" size="3">Back to Home</font></a><font face="Times New Roman" size="3">]

<hr>

<h2>Publicaitons</h3>
<!--[Conference] [Journal] [Others]-->

<h4>Conference</h4>

<ul>

<li>O. Nachum, Y. Chow, B. Dai, and <b>L. Li</b>: DualDICE: Behavior-agnostic estimation of discounted stationary distribution dorrections. In <i>Advances in Neural Information Processing Systems 32 (NeurIPS)</i>, 2019. [<a href="http://arxiv.org/abs/1906.04733">arXiv</a>]

<li>Y. Feng, <b>L. Li</b>, and Q. Liu: A kernel loss for solving the Bellman equation.  In <i>Advances in Neural Information Processing Systems 32 (NeurIPS)</i>, 2019. [<a href="http://arxiv.org/abs/1905.10506">arXiv</a>]

<li>C. Dann, <b>L. Li</b>, W. Wei, and E. Brunskill: Policy certificates: Towards accountable reinforcement learning.  In <i>the 36th International Conference on Machine Learning (ICML)</i>, 2019. [<a href="http://proceedings.mlr.press/v97/dann19a.html">link</a>]

<li><b>L. Li</b>: A perspective on off-policy evaluation in reinforcement learning. Frontiers of Computer Science, 13(5):911-912, 2019. [<a href="https://rd.springer.com/article/10.1007%2Fs11704-019-9901-7">link</a>]

<li>H. Dong, J. Mao, T. Lin, C. Wang, <b>L. Li</b>, and D. Zhou: Neural logic machines.  In <i>the 7th International Conference on Learning Representations (ICLR)</i>, 2019. [<a href="https://arxiv.org/abs/1904.11694">arXiv</a>]

<li>Q. Liu, <b>L. Li</b>, Z. Tang, and D. Zhou: Breaking the curse of horizon: Infinite-horizon off-policy estimation.  In <i>Advances in Neural Information Processing Systems 31 (NIPS)</i>, 2018. [<a href="http://papers.nips.cc/paper/7781-breaking-the-curse-of-horizon-infinite-horizon-off-policy-estimation">link</a>]

<li>K.-S. Jun, <b>L. Li</b>, Y. Ma, and J. Zhu: Adversarial attacks on stochastic bandits.  In <i>Advances in Neural Information Processing Systems 31 (NIPS)</i>, 2018. [<a href="http://papers.nips.cc/paper/7622-adversarial-attacks-on-stochastic-bandits">link</a>]

<li>Y. Ma, K.-S. Jun, <b>L. Li</b>, and J. Zhu: Data poisoning attacks in contextual bandits.  In <i>the 9th Conference on Decision and Game Theory for Security (GameSec)</i>, 2018. [<a href="http://arxiv.org/abs/1808.05760">arXiv</a>]

<li>D. Tang, X. Li, J. Gao, C. Wang, <b>L. Li</b>, and T. Jebara: Subgoal discovery for hierarchical dialogue policy learning.  In <i>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, 2018. [<a href="https://aclanthology.info/papers/D18-1253/d18-1253">link</a>]

<li>B. Dai, A. Shaw, <b>L. Li</b>, L. Xiao, N. He, Z. Liu, J. Chen, and L. Song: SBEED: Convergent reinforcement learning with nonlinear function approximation.  In <i>the 35th International Conference on Machine Learning (ICML)</i>, 2018. [<a href="http://proceedings.mlr.press/v80/dai18c.html">link</a>, <a href="https://arxiv.org/abs/1712.10285">arXiv</a>, <a href="slides/slides_dai18sbeed.pdf">slides</a>]

<li>Y. Chen, <b>L. Li</b>, and M. Wang: Bilinear Ï€ learning using state and action features.  In <i>the 35th International Conference on Machine Learning (ICML)</i>, 2018. [<a href="http://proceedings.mlr.press/v80/chen18e.html">link</a>, <a href="https://arxiv.org/abs/1804.10328">arXiv</a>]

<li>B. Dai, A. Shaw, N. He, <b>L. Li</b>, and L. Song: Boosting the actor with dual critic.  In <i>the 6th International Conference on Learning Representations (ICLR)</i>, 2018. [<a href="https://arxiv.org/abs/1712.10282">arXiv</a>]

<li>Z. Lipton, X. Li, J. Gao, <b>L. Li</b>, F. Ahmed, and L. Deng: Efficient dialogue policy learning with BBQ-networks.  In <i>the 32nd AAAI Conference on Artificial Intelligence (AAAI)</i>, 2018. [<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16189">link</a>]

<li>J. Chen, C. Wang, L. Xiao, J. He, <b>L. Li</b>, and L. Deng: Q-LDA: Uncovering latent patterns in text-based sequential decision processes.  In <i>Advances in Neural Information Processing Systems 30 (NIPS)</i>, 2017.  [<a href="http://papers.nips.cc/paper/7083-q-lda-uncovering-latent-patterns-in-text-based-sequential-decision-processes">link</a>]

<li>B. Peng, X. Li, <b>L. Li</b>, J. Gao, A. Celikyilmaz, S. Lee, and K.-F. Wong: Composite task-completion dialogue system via hierarchical deep reinforcement learning.  In <i>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, 2017. [<a href="https://aclanthology.info/papers/D17-1237/d17-1237">link</a>]

<li><b>L. Li</b>, Y. Lu, and D. Zhou: Provably optimal algorithms for generalized linear contextual bandits.  In <i>the 34th International Conference on Machine Learning (ICML)</i>, 2017. [<a href="http://proceedings.mlr.press/v70/li17c.html">link</a>]

<li>S. Du, J. Chen, <b>L. Li</b>, L. Xiao, and D. Zhou: Stochastic variance reduction methods for policy evaluation.  In <i>the 34th International Conference on Machine Learning (ICML)</i>, 2017. [<a href="http://proceedings.mlr.press/v70/du17a.html">link</a>]

<li>B. Dhingra, <b>L. Li</b>, X. Li, J. Gao, Y.-N. Chen, F. Ahmed, and L. Deng: Towards end-to-end reinforcement learning of dialogue agents for information access.  In <i>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</i>, 2017. [<a href="https://doi.org/10.18653/v1/P17-1045">link</a>]

<li>E. Parisotto, A. Mohamed, R. Singh, <b>L. Li</b>, D. Zhou, and P. Kohli: Neuro-symbolic program synthesis.  In <i>the 5th International Conference on Learning Representations (ICLR)</i>, 2017. [<a href="http://arxiv.org/abs/1611.01855">link</a>]

<li>T.K. Huang, <b>L. Li</b>, A. Vartanian, S. Amershi, and J. Zhu: Active learning with oracle epiphany.  In Advances in Neural Information Processing Systems 29 (NIPS), 2016. [<a href="http://papers.nips.cc/paper/6155-active-learning-with-oracle-epiphany">link</a>]

<li>J. He, M. Ostendorf, X. He, J. Chen, J. Gao, <b>L. Li</b>, and L. Deng: Deep reinforcement learning with a combinatorial action space for predicting and tracking popular discussion threads.  In <i>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, 2016. [<a href="http://www.aclweb.org/anthology/D16-1189">link</a>]

<li>C.-Y. Liu and <b>L. Li</b>: On the Prior Sensitivity of Thompson Sampling. In <i>the 27th International Conference on Algorithmic Learning Theory (ALT)</i>, 2016. [<a  href="https://doi.org/10.1007/978-3-319-46379-7_22">link</a>]

<li>J. He, J. Chen, X. He, J. Gao, <b>L. Li</b>, L. Deng, and M. Ostendorf: Deep reinforcement learning with a natural language action space.  In <i>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</i>, 2016. [<a href="http://aclweb.org/anthology/P/P16/P16-1153.pdf">link</a>]

<li>N. Jiang and <b>L. Li</b>: Doubly robust off-policy value evaluation for reinforcement learning.  In <i>the 33rd International Conference on Machine Learning (ICML)</i>, 2016. [<a href="http://jmlr.org/proceedings/papers/v48/jiang16.html">link</a>]

<li>S. Agrawal, N. R. Devanur, and <b>L. Li</b>: An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives.  In <i>the 29th Annual Conference on Learning Theory (COLT)</i>, 2016. [<a href="http://jmlr.org/proceedings/papers/v49/agrawal1OA6.html">link</a>]

<li>M. Zoghi, T. Tunys, <b>L. Li</b>, D. Jose, J. Chen, C.-M. Chin, and M. de Rijke: Click-based hot fixes for underperforming torso queries.  In <i>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</i>, 2016. [<a href="https://dl.acm.org/citation.cfm?id=2911500">link</a>]

<li>J. He, J. Chen, X. He, J. Gao, <b>L. Li</b>, L. Deng, and M. Ostendorf: Deep reinforcement learning with an unbounded action space.  In <i>the International Conference on Learning Representations (ICLR), Workshop Track</i>, 2016.

<li><b>L. Li</b>, R. Munos, and Cs. Szepesvari: Toward minimax off-policy value estimation.  In <i>the 18th International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2015. [<a href="http://jmlr.org/proceedings/papers/v38/li15b.html">link</a>]

<li><b>L. Li</b>, S. Chen, J. Kleban, and A. Gupta: Counterfactual estimation and optimization of click metrics in search engines: A case study.  In <i>the 24th International Conference on World Wide Web (WWW), Companion</i>, 2015. [<a href="http://doi.acm.org/10.1145/2740908.2742562">link</a>]

<li><b>L. Li</b>, J. Kim, and I. Zitouni: Toward predicting the outcome of an A/B experiment for search relevance.  In <i>the 8th International Conference on Web Search and Data Mining (WSDM)</i>, 2015. [<a href="http://doi.acm.org/10.1145/2684822.2685311">link</a>]

<li><b>L. Li</b>, H. He, and J.D. Williams: Temporal supervised learning for inferring a dialog policy from example conversations.  In the <i>IEEE Spoken Language Technology Workshop (SLT)</i>, 2014.

<li>A. Agarwal, D. Hsu, S. Kale, J. Langford, <b>L. Li</b>, and R.E. Schapire: Taming the monster: A fast and simple algorithm for contextual bandits.  In <i>the 31st International Conference on Machine Learning (ICML)</i>, 2014.

<li>E. Brunskill and <b>L. Li</b>: PAC-inspired option discovery in lifelong reinforcement learning.  In <i>the 31st International Conference on Machine Learning (ICML)</i>, 2014.

<li>E. Brunskill and <b>L. Li</b>: Sample complexity of multi-task reinforcement learning.  In <i>the 29th Conference on Uncertainty in Artificial Intelligence (UAI)</i>, 2013.

<li>M. Dudik, D. Erhan, J. Langford, and <b>L. Li</b>: Sample-efficient nonstationary-policy evaluation for contextual bandits.  In <i>the 28th Conference on Uncertainty in Artificial Intelligence (UAI)</i>, 2012.

<li><b>L. Li</b>, W. Chu, J. Langford, T. Moon, and X. Wang: An unbiased offline evaluation of contextual bandit algorithms with generalized linear models.  In <i>Journal of Machine Learning Research - Workshop and Conference Proceedings 26: On-line Trading of Exploration and Exploitation 2</i>, 2012.

<li>V. Navalpakkam, R. Kumar, <b>L. Li</b>, and D. Sivakumar: Attention and selection in online choice tasks.  In <i>the 20th International Conference on User Modeling, Adaptation and Personalization (UMAP)</i>, 2012

<li>H. Wang, A. Dong, <b>L. Li</b>, Y. Chang, and E. Gabrilovich: Joint relevance and freshness learning From clickthroughs for news search.  In <i>the 21st International Conference on World Wide Web (WWW)</i>, 2012.

<li>O. Chapelle and <b>L. Li</b>: An empirical evaluation of Thompson sampling.  In <i>Advances in Neural Information Processing Systems 24 (NIPS)</i>, 2011.

<li>M. Dudik, J. Langford, and <b>L. Li</b>: Doubly robust policy evaluation and learning.  In <i>the 28th International Conference on Machine
Learning (ICML)</i>, 2011.

<li>W. Chu, M. Zinkevich, <b>L. Li</b>, A. Thomas, and B. Tseng: Unbiased online active learning in data streams.  In <i>the 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</i>, 2011.

<li>D. Agarwal, <b>L. Li</b>, and A.J. Smola: Linear-time algorithms for propensity scores.  In <i>the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2011.

<li>A. Beygelzimer, J. Langford, <b>L. Li</b>, L. Reyzin, and R.E. Schapire: Contextual bandit algorithms with supervised learning guarantees.  In <i>the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2011.  <b>Co-winner of the Notable Paper Award.</b>

<li>W. Chu, <b>L. Li</b>, L. Reyzin, and R. Schapire: Linear contextual bandit problems.  In <i>the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2011.

<li><b>L. Li</b>, Wei Chu, John Langford, and Xuanhui Wang: Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms.  In <i>the 4th ACM International Conference on Web Search and Data Mining (WSDM)</i>, 2011.  <b>Winner of the Best Paper Award.</b>

<li>A.L. Strehl, J. Langford, <b>L. Li</b>, and S. Kakade: Learning from logged implicit exploration data.  In <i>Advances in Neural Information Processing Systems 23 (NIPS)</i>, 2011.

<li>M. Zinkevich, M. Weimer, A.J. Smola, and <b>L. Li</b>: Convergence rates of parallel online learning via stochastic gradient descent.  In <i>Advances in Neural Information Processing Systems 23 (NIPS)</i>, 2011.

<li>T. Moon, <b>L. Li</b>, W. Chu, C. Liao, Z. Zheng, and Y. Chang:  Online learning for recency search ranking using real-time user feedback (short paper).  In <i>the 19th ACM Conference on Information and Knowledge Management (CIKM)</i>, 2010.

<li><b>L. Li</b>, W. Chu, J. Langford, and R.E. Schapire: A contextual-bandit approach to personalized news article recommendation.  In <i>the 19th International Conference on World Wide Web (WWW)</i>, 2010.

<!--<li>Y. Xie, Y. Zhang, and <b>L. Li</b>: Neuro-fuzzy reinforcement learning for adaptive intersection traffic signal control.  In <i>the Annual Meeting of Transportation Research Board (TRB)</i>, 2010.-->

<li><b>L. Li</b>, J.D. Williams, and S. Balakrishnan: Reinforcement learning for spoken dialog management using least-squares policy iteration and fast feature selection. In <i>the 10th Annual Conference of the International Speech Communication Association (INTERSPEECH)</i>, 2009.

<li>C. Diuk, <b>L. Li</b>, and B.R. Leffler: The adaptive $k$-meteorologists problem and its application to structure learning and feature selection in reinforcement learning. In <i>the 26th International Conference on Machine Learning (ICML)</i>, 2009.

<li>J. Asmuth, <b>L. Li</b>, M.L. Littman, A. Nouri, and D. Wingate: A Bayesian sampling approach to exploration in reinforcement learning.  In <i>the 25th International Conference on Uncertainty in Artificial Intelligence (UAI)</i>, 2009.

<li><b>L. Li</b>, M.L. Littman and C.R. Mansley: Online exploration in least-squares policy iteration. In <i>the 8th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</i>, 2009</b>.

<li>L. Langford, <b>L. Li</b>, and T. Zhang: Sparse online learning via truncated gradient.  In <i>Advances in Neural Information Processing Systems 21 (NIPS)</i>, 2009.

<li><b>L. Li</b>: A worst-case comparison between temporal difference and residual gradient.  In <i>the 25th International Conference on Machine Learning (ICML)</i>, 2008.

<li><b>L. Li</b>, M.L. Littman, and T.J. Walsh: Knows what it knows: A framework for self-aware learning. In <i>the 25th International Conference on Machine Learning (ICML)</i>, 2008.  <b>Co-winner of the Best Student Paper Award. A Google Student Award winner at the New York Academy of Sciences Symposium on Machine Learning, 2008.</b>

<li>R. Parr, <b>L. Li</b>, G. Taylor, C. Painter-Wakefield, and M.L. Littman: An analysis of linear models, linear value function approximation, and feature selection for reinforcement learning. In <i>the 25th International Conference on Machine Learning (ICML)</i>, 2008.

<li>E. Brunskill, B.R. Leffler, <b>L. Li</b>, M.L. Littman, and N. Roy: CORL: A continuous-state offset-dynamics reinforcement learner.  In <i>the 24th Conference on Uncertainty in Artificial Intelligence (UAI)</i>, 2008.

<li><b>L. Li</b> and M.L. Littman: Efficient value-function approximation via online linear regression.  In <i>the 10th International Symposium on Artificial Intelligence and Mathematics (AI&Math)</i>, 2008.

<li>J. Wortman, Y. Vorobeychik, <b>L. Li</b>, and J. Langford: Maintaining equilibria during exploration in sponsored search auctions.  In <i>the 3rd International Workshop on Internet and Network Economics (WINE)</i>, LNCS 4858, 2007.

<li>T.J. Walsh, A. Nouri, <b>L. Li</b>, and M.L. Littman: Planning and learning in environments with delayed feedback.  In <i>the 18th European Conference on Machine Learning (ECML)</i>, LNCS 4701, 2007.

<li>R. Parr, C. Painter-Wakefield, <b>L. Li</b>, and M.L. Littman: Analyzing feature generation for value-function approximation.  In <i>the 24th International Conference on Machine Learning (ICML)</i>, 2007.

<li>A.L. Strehl, <b>L. Li</b>, E. Wiewiora, J. Langford, and M.L. Littman: PAC model-free reinforcement learning.  In <i>the 23rd International Conference on Machine Learning (ICML)</i>, 2006.  <b>Best Student Poster Award winner at the New York Academy of Sciences Symposium on Machine Learning, 2006.</b>

<li>A.L. Strehl, <b>L. Li</b>, and M.L. Littman: Incremental model-based learners with formal learning-time guarantees.  In <i>the 22nd Conference on Uncertainty in Artificial Intelligence (UAI)</i>, 2006.

<li><b>L. Li</b>, T.J. Walsh, and M.L. Littman: Towards a unified theory of state abstraction for MDPs. In <i>the 9th International Symposium on Artificial Intelligence and Mathematics (AI&Math)</i>, 2006.

<li><b>L. Li</b>, M.L. Littman: Lazy approximation for solving continuous finite-horizon MDPs. In <i>the 20th National Conference on Artificial Intelligence (AAAI)</i>, 2005.

<li><b>L. Li</b>, V. Bulitko, and R. Greiner: Batch reinforcement learning with state importance (extended abstract).  In <i>the 15th European Conference on Machine Learning (ECML)</i>, LNCS 3201, 2004.

<li>V. Bulitko, <b>L. Li</b>, R. Greiner, and I. Levner: Lookahead pathologies for single agent search (poster paper). In <i>the 18th International Joint Conference on Artificial Intelligence (IJCAI)</i>, 2003.

<li>I. Levner, V. Bulitko, <b>L. Li</b>, G. Lee, and R. Greiner: Towards automated creation of image interpretation systems. In <i>the 16th Australian Joint Conference on Artificial
Intelligence</i>, LNCS 2903, 2003.

<li><b>L. Li</b>, V. Bulitko, R. Greiner, and I. Levner: Improving an adaptive image interpretation system by leveraging. In <i>the 8th Australian and New Zealand Intelligent Information System Conference</i>, 2003.

</ul>

<h4>Journal</h4>

<ul>

<li>M. Dudik, D. Erhan, J. Langford, and <b>L. Li</b>: Doubly robust policy evaluation and optimization.  In <i>Statistical Science</i>, 29(4):485--511, 2014.

<li>J. Bian, B. Long, <b>L. Li</b>, T. Moon, A. Dong, and Y. Chang: Exploiting user preference for online learning in Web content optimization systems.  In <i>ACM Transactions on Intelligent Systems and Technology</i>, 5(2), 2014.

<li>T. Moon, W. Chu, <b>L. Li</b>, Z. Zheng, and Y. Chang: Refining recency search results with user click feedback.  In <i>ACM Transactions on Information Systems</i>, 30(4), 2012.

<li>J. Langford, <b>L. Li</b>, P. McAfee, and K. Papineni: Cloud control: Voluntary admission control for Intranet traffic management.  In <i>Information Systems and e-Business Management</i>, 10(3):295--308, 2012.

<li><b>L. Li</b>, M.L. Littman, T.J. Walsh, and A.L. Strehl: Knows what it knows: A framework for self-aware learning.  In <i>Machine Learning</i>, 82(3):399--443, 2011.

<li><b>L. Li</b> and M.L. Littman: Reducing reinforcement learning to KWIK online regression.  In the <i>Annals of Mathematics and Artificial Intelligence</i>, 58(3--4):217--237, 2010.

<li>J. Langford, <b>L. Li</b>, J. Wortman, and Y. Vorobeychik: Maintaining equilibria during exploration in sponsored search auctions.  In <i>Algorithmica</i>, 58(4):990--1021, 2010.

<li>A.L. Strehl, <b>L. Li</b>, and M.L. Littman: Reinforcement learning in finite MDPs: PAC analysis.  In the <i>Journal of Machine Learning Research</i>, 10:2413--2444, 2009.

<li>E. Brunskill, B.R. Leffler, <b>L. Li</b>, M.L. Littman, and N. Roy: Provably efficient learning with typed parametric models.  In the <i>Journal of Machine Learning Research</i>, 10:1955--1988, 2009.

<li>J. Langford, <b>L. Li</b>, and T. Zhang: Sparse online learning via truncated gradient. In the <i>Journal of Machine Learning Research</i>, 10:777--801, 2009.

<li>T.J. Walsh, A. Nouri, <b>L. Li</b>, and M.L. Littman: Planning and learning in environments with delayed feedback. In the <i>Journal of Autonomous Agents and Multi-Agent Systems</i>, 18(1):83--105, 2009.

<li><b>L. Li</b>, V. Bulitko, and R. Greiner: Focus of attention in reinforcement learning. In the <i>Journal of Universal Computer Science</i>, 13(9):1246--1269, 2007.

<!--<li><b>L. Li</b>, M. , Z. Zheng, C. He, and Z.-H. Du: Typical XML document transformation methods and an application system (in Chinese). <i>Computer Science</i>, 30(2):40--44, February, 2003.-->

</ul>

<h4>Theses, Surveys, Books, and Chapters</h4>

<ul>

<li>J. Gao, M. Galley, and <b>L. Li</b>: Neural approaches to Conversational AI: Question answering, task-oriented dialogues and social chatbots.  Foundations and Trends in Information Retrieval.  [<a href="https://www.nowpublishers.com/article/Details/INR-074">official</a>, <a href="https://arxiv.org/abs/1809.08267">draft version</a>]

<li>K. Hofmann, <b>L. Li</b>, and F. Radlinski: Online Evaluation for Information Retrieval.  Foundations and Trends in Information Retrieval, 10(1):1--107, 2016. ISBN 978-1-68083-163-4.

<li><b>L. Li</b>: Sample complexity bounds of exploration.  In Marco Wiering and Martijn van Otterlo, editors, <i>Reinforcement Learning: State of the Art</i>, Springer Verlag, 2012.

<li><b>L. Li</b>: A unifying framework for computational reinforcement learning theory. <i>Doctoral dissertation</i>, Department of Computer Science, Rutgers University, New Brunswick, NJ, USA, May, 2009.

<li><b>L. Li</b>: Focus of attention in reinforcement learning. <i>MSc thesis</i>, Department of Computing Science, University
of Alberta, Edmonton, Alberta, Canada, July, 2004.

</ul>

<h4>Others</h4>

<ul>

<li>X. Li, Z.C. Lipton, B. Dhingra, <b>L. Li</b>, J. Gao, Y.-N. Chen: A user simulator for task-completion dialogues.  MSR technical report, December 2016.

<li>E. Brunskill and <b>L. Li</b>: The online discovery problem and its application to lifelong reinforcement learning.  CoRR abs/1506.03379, June 2015.

<li>D. Yankov, P. Berkhin, and <b>L. Li</b>: Evaluation of explore-exploit policies in multi-result ranking systems.  Microsoft Journal on Applied Research, volume 3, pages 54--60, 2015.  Also available as Microsoft Research Technical Report MSR-TR-2015-34, May 2015.

<li>Z. Qin, V. Petricek, N. Karampatziakis, <b>L. Li</b>, and J. Langford: Efficient online bootstrapping for large scale learning.  <i>NIPS Workshop on Big Data</i>, December, 2013.  Also available as Microsoft Research Technical Report MSR-TR-2013-132.

<li><b>L. Li</b> and O. Chapelle: Regret bounds for Thompson sampling (Open Problems).  In the <i>Twenty-Fifth Annual Conference on Learning Theory (COLT)</i>, 2012

<li><b>L. Li</b> and M.L. Littman: Prioritized sweeping converges to the optimal value function.  Technical report DCS-TR-631, Department of Computer Science, Rutgers University, May 2008.

<li>A.L. Strehl, <b>L. Li</b>, and M.L. Littman: PAC reinforcement learning bounds for RTDP and Rand-RTDP.  <i>AAAI technical report WS-06-11</i>, pages 50-56, July 2006.

<li><b>L. Li</b> and M.L. Littman: Lazy approximation: A new approach for solving continuous finite-horizon MDPs.  Technical report DCS-TR-577, Department of Computer Science, Rutgers University, May 2005.

</ul>

</body>

</html>
