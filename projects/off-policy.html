<html>

<head>
<title>Lihong Li</title>
<meta name="author" content="Lihong Li">
<meta name="description" content="Personal Homepage">
<meta name="keywords" content="Artificial Intelligence, Machine Learning, Reinforcement Learning, Planning, Research, Rutgers, Yahoo!, Microsoft, Google">
<meta name="classification" content="Personal Homepage">
</head>

<body bgcolor="f7f7f7" text="black" link="blue" vlink="purple" alink="red">

<h1> <img src="../img/lihong-pro.jpg"> Lihong Li <img src="../img/chinese-name.jpg" height="25"></h1>

Research Scientist <br/>
Google Inc. <br/><br/>
lihongli.cs@gmail.com (for general academic work) <br/>
lihong@google.com (for Google related work) <br/>
747 Sixth Street South, Kirkland, WA, USA 98033<br/>
<p>

<p/><hr>

[<a href="../index.html#interests"><font face="Times New Roman" size="3">Interests</font></a><font face="Times New Roman" size="3">]
[<a href="../index.html#experiences"><font face="Times New Roman" size="3">Experiences</font></a><font face="Times New Roman" size="3">]
[</font><a href="../pubs.html" target="_self"><font face="Times New Roman" size="3">Publications</font></a><font face="Times New Roman" size="3">]
[<a href="../index.html#activities"><font face="Times New Roman" size="3">Professional Activities</font></a><font face="Times New Roman" size="3">]
[</font><a href="../index.html" target="_self"><font face="Times New Roman" size="3">Back to Home</font></a><font face="Times New Roman" size="3">]

<p/><hr>

<h2>Off-policy Learning</h2>

How can we evaluate the quality of a new policy using data collected by another policy?  Answer to this question finds a wide range of applications in the industry, alleviating the need for frequent online experimentation that can be costly, time-consuming, and risky.  This problem is very related to covariate-shift and causal effect estimation.

<ul>

<li>Q. Liu, <b>L. Li</b>, Z. Tang, and D. Zhou: Breaking the curse of horizon: Infinite-horizon off-policy estimation.  In <i>Advances in Neural Information Processing Systems 31 (NIPS)</i>, 2018. [<a href="../papers/liu18breaking.pdf">
  </a>]

<li>N. Jiang and <b>L. Li</b>: Doubly robust off-policy value evaluation for reinforcement learning.  In <i>the 33rd International Conference on Machine Learning (ICML)</i>, 2016. [<a href="http://jmlr.org/proceedings/papers/v48/jiang16.html">link</a>]

<li>M. Zoghi, T. Tunys, <b>L. Li</b>, D. Jose, J. Chen, C.-M. Chin, and M. de Rijke: Click-based hot fixes for underperforming torso queries.  In <i>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</i>, 2016. [<a href="https://dl.acm.org/citation.cfm?id=2911500">link</a>]

<li>K. Hofmann, <b>L. Li</b>, and F. Radlinski: Online Evaluation for Information Retrieval.  Foundations and Trends in Information Retrieval, 10(1):1--107, 2016. ISBN 978-1-68083-163-4.

<li><b>L. Li</b>, R. Munos, and Cs. Szepesvari: Toward minimax off-policy value estimation.  In <i>the 18th International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2015. [<a href="http://jmlr.org/proceedings/papers/v38/li15b.html">link</a>]

<li><b>L. Li</b>, S. Chen, J. Kleban, and A. Gupta: Counterfactual estimation and optimization of click metrics in search engines: A case study.  In <i>the 24th International Conference on World Wide Web (WWW), Companion</i>, 2015. [<a href="http://doi.acm.org/10.1145/2740908.2742562">link</a>]

<li><b>L. Li</b>, J. Kim, and I. Zitouni: Toward predicting the outcome of an A/B experiment for search relevance.  In <i>the 8th International Conference on Web Search and Data Mining (WSDM)</i>, 2015. [<a href="http://doi.acm.org/10.1145/2684822.2685311">link</a>]

<li>D. Yankov, P. Berkhin, and <b>L. Li</b>: Evaluation of explore-exploit policies in multi-result ranking systems.  Microsoft Journal on Applied Research, volume 3, pages 54--60, 2015.  Also available as Microsoft Research Technical Report MSR-TR-2015-34, May 2015.

<li>M. Dudik, D. Erhan, J. Langford, and <b>L. Li</b>: Doubly robust policy evaluation and optimization.  In <i>Statistical Science</i>, 29(4):485--511, 2014.

<li>M. Dudik, D. Erhan, J. Langford, and <b>L. Li</b>: Sample-efficient nonstationary-policy evaluation for contextual bandits.  In <i>the 28th Conference on Uncertainty in Artificial Intelligence (UAI)</i>, 2012.

<li><b>L. Li</b>, W. Chu, J. Langford, T. Moon, and X. Wang: An unbiased offline evaluation of contextual bandit algorithms with generalized linear models.  In <i>Journal of Machine Learning Research - Workshop and Conference Proceedings 26: On-line Trading of Exploration and Exploitation 2</i>, 2012.

<li>M. Dudik, J. Langford, and <b>L. Li</b>: Doubly robust policy evaluation and learning.  In <i>the 28th International Conference on Machine
Learning (ICML)</i>, 2011.

<li>D. Agarwal, <b>L. Li</b>, and A.J. Smola: Linear-time algorithms for propensity scores.  In <i>the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2011.

<li><b>L. Li</b>, Wei Chu, John Langford, and Xuanhui Wang: Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms.  In <i>the 4th ACM International Conference on Web Search and Data Mining (WSDM)</i>, 2011.

<li>A.L. Strehl, J. Langford, <b>L. Li</b>, and S. Kakade: Learning from logged implicit exploration data.  In <i>Advances in Neural Information Processing Systems 23 (NIPS)</i>, 2011.

</ul>

</html>
