<html>

<script src="../header.js"></script>

[<a href="../index.html#interests"><font face="Times New Roman" size="3">Interests</font></a><font face="Times New Roman" size="3">]
[<a href="../index.html#experiences"><font face="Times New Roman" size="3">Experiences</font></a><font face="Times New Roman" size="3">]
[</font><a href="../pubs.html" target="_self"><font face="Times New Roman" size="3">Publications</font></a><font face="Times New Roman" size="3">]
[<a href="../index.html#activities"><font face="Times New Roman" size="3">Professional Activities</font></a><font face="Times New Roman" size="3">]
[</font><a href="../index.html" target="_self"><font face="Times New Roman" size="3">Back to Home</font></a><font face="Times New Roman" size="3">]

<p/><hr>

<h2>Off-policy Learning</h2>

How can we evaluate the quality of a new policy using data collected by another policy?  Answer to this question finds a wide range of applications in the industry, alleviating the need for frequent online experimentation that can be costly, time-consuming, and risky.  This problem is very related to covariate-shift and causal effect estimation.

<ul>

<li>Z. Tang, Y. Duan, S. Zhu, S. Zhang, and <b>L. Li</b>: Estimating long-term effects from experimental data. In <i>the 16th ACM Conference on Recommender Systems (RecSys), Industry Track, 2022.}

<li>C. Xiao, Y. Wu, T. Lattimore, B. Dai, J. Mei, <b>L. Li</b>, Cs. Szepesvari, and D. Schuurmans: On the optimality of batch policy optimization algorithms. In <i>the 38th International Conference on Machine Learning (ICML)</i>, 2021. [<a href="https://arxiv.org/abs/2104.02293">arXiv</a>]

<li>A. Bennett, N. Kallus, <b>L. Li</b>, and A. Mousavi: Off-policy evaluation in infinite-horizon reinforcement learning with latent confounders.  In <i>the 24th International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2021.  [<a href="https://arxiv.org/abs/2007.13893">arXiv</a>]

<li>O. Nachum, B. Dai, I. Kostrikov, Y. Chow, <b>L. Li</b>, and D. Schuurmans: AlgaeDICE: Policy gradient from arbitrary experience.  [<a href="http://arxiv.org/abs/1912.02074">arXiv</a>]

<li>B. Dai, O. Nachum, Y. Chow, <b>L. Li</b>, Cs. Szepesvari, D. Schuurmans: CoinDICE: Off-policy confidence interval estimation.  In <i>Advances in Neural Information Processing Systems 33 (NeurIPS)</i>, spotlight, 2020.

<li>M. Yang, O. Nachum, B. Dai, <b>L. Li</b>, D. Schuurmans: Off-policy evaluation via the regularized Lagrangian.  In <i>Advances in Neural Information Processing Systems 33 (NeurIPS)</i>, 2020.  [<a href="https://arxiv.org/abs/2007.03438">arXiv</a>]

<li>J. Wen, B. Dai, <b>L. Li</b>, and D. Schuurmans: Batch stationary distribution estimation.  In <i>the 37th International Conference on Machine Learning (ICML)</i>, 2020. [<a href="https://arxiv.org/abs/2003.00722">arXiv</a>]

<li>R. Zhang, B. Dai, <b>L. Li</b>, and D. Schuurmans: GenDICE: Generalized offline estimation of stationary values.  In <i>the 8th International Conference on Learning Representations (ICLR)</i>, 2020. [<a href="https://openreview.net/pdf?id=HkxlcnVFwB">link</a>, <a href="https://arxiv.org/abs/2002.09072">arXiv</a>]

<li>Z. Tang, Y. Feng, <b>L. Li</b>, D. Zhou, and Q. Liu: Doubly robust bias reduction in infinite horizon off-policy estimation.  In <i>the 8th International Conference on Learning Representations (ICLR)</i>, 2020. [<a href="https://openreview.net/pdf?id=S1glGANtDr">link</a>]

<li>A. Mousavi, <b>L. Li</b>, Q. Liu, and D. Zhou: Black-box off-policy estimation for infinite-horizon reinforcement learning.  In <i>the 8th International Conference on Learning Representations (ICLR)</i>, 2020. [<a href="https://openreview.net/pdf?id=S1ltg1rFDS">link</a>, <a href="https://arxiv.org/abs/2003.11126">arXiv</a>]
  
<li>O. Nachum, Y. Chow, B. Dai, and <b>L. Li</b>: DualDICE: Behavior-agnostic estimation of discounted stationary distribution corrections. In <i>Advances in Neural Information Processing Systems 32 (NeurIPS)</i>, spotlight, 2019. [<a href="http://arxiv.org/abs/1906.04733">arXiv</a>]

<li><b>L. Li</b>: A perspective on off-policy evaluation in reinforcement learning (Invited Paper).  Frontiers of Computer Science, 13(5):911-912, 2019.  [<a href="https://rd.springer.com/article/10.1007%2Fs11704-019-9901-7">link</a>, <a href="../papers/li19perspective.pdf">PDF</a>]

<li>Q. Liu, <b>L. Li</b>, Z. Tang, and D. Zhou: Breaking the curse of horizon: Infinite-horizon off-policy estimation.  In <i>Advances in Neural Information Processing Systems 31 (NeurIPS)</i>, spotlight, 2018. [<a href="http://papers.nips.cc/paper/7781-breaking-the-curse-of-horizon-infinite-horizon-off-policy-estimation">link</a>]

<li>N. Jiang and <b>L. Li</b>: Doubly robust off-policy value evaluation for reinforcement learning.  In <i>the 33rd International Conference on Machine Learning (ICML)</i>, 2016. [<a href="http://jmlr.org/proceedings/papers/v48/jiang16.html">link</a>]

<li>M. Zoghi, T. Tunys, <b>L. Li</b>, D. Jose, J. Chen, C.-M. Chin, and M. de Rijke: Click-based hot fixes for underperforming torso queries.  In <i>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</i>, 2016. [<a href="https://dl.acm.org/citation.cfm?id=2911500">link</a>]

<li>K. Hofmann, <b>L. Li</b>, and F. Radlinski: Online Evaluation for Information Retrieval.  Foundations and Trends in Information Retrieval, 10(1):1--107, 2016. ISBN 978-1-68083-163-4. [<a href="https://www.nowpublishers.com/article/Details/INR-051">link</a>, <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/ftir-online-evaluation-final-journal.pdf">PDF</a>]

<li><b>L. Li</b>, R. Munos, and Cs. Szepesvari: Toward minimax off-policy value estimation.  In <i>the 18th International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2015. [<a href="http://jmlr.org/proceedings/papers/v38/li15b.html">link</a>]

<li><b>L. Li</b>, S. Chen, J. Kleban, and A. Gupta: Counterfactual estimation and optimization of click metrics in search engines: A case study.  In <i>the 24th International Conference on World Wide Web (WWW), Companion</i>, 2015. [<a href="http://doi.acm.org/10.1145/2740908.2742562">link</a>]

<li><b>L. Li</b>, J. Kim, and I. Zitouni: Toward predicting the outcome of an A/B experiment for search relevance.  In <i>the 8th International Conference on Web Search and Data Mining (WSDM)</i>, 2015. [<a href="http://doi.acm.org/10.1145/2684822.2685311">link</a>]

<li>D. Yankov, P. Berkhin, and <b>L. Li</b>: Evaluation of explore-exploit policies in multi-result ranking systems.  Microsoft Journal on Applied Research, volume 3, pages 54--60, 2015.  Also available as Microsoft Research Technical Report MSR-TR-2015-34, May 2015.

<li>M. Dudik, D. Erhan, J. Langford, and <b>L. Li</b>: Doubly robust policy evaluation and optimization.  In <i>Statistical Science</i>, 29(4):485--511, 2014.

<li>M. Dudik, D. Erhan, J. Langford, and <b>L. Li</b>: Sample-efficient nonstationary-policy evaluation for contextual bandits.  In <i>the 28th Conference on Uncertainty in Artificial Intelligence (UAI)</i>, 2012.

<li><b>L. Li</b>, W. Chu, J. Langford, T. Moon, and X. Wang: An unbiased offline evaluation of contextual bandit algorithms with generalized linear models.  In <i>Journal of Machine Learning Research - Workshop and Conference Proceedings 26: On-line Trading of Exploration and Exploitation 2</i>, 2012.

<li>M. Dudik, J. Langford, and <b>L. Li</b>: Doubly robust policy evaluation and learning.  In <i>the 28th International Conference on Machine
Learning (ICML)</i>, 2011.

<li>D. Agarwal, <b>L. Li</b>, and A.J. Smola: Linear-time algorithms for propensity scores.  In <i>the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2011.

<li><b>L. Li</b>, W. Chu, J. Langford, and X. Wang: Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms.  In <i>the 4th ACM International Conference on Web Search and Data Mining (WSDM)</i>, 2011.

<li>A.L. Strehl, J. Langford, <b>L. Li</b>, and S. Kakade: Learning from logged implicit exploration data.  In <i>Advances in Neural Information Processing Systems 23 (NIPS)</i>, spotlight, 2011.

</ul>

</html>
