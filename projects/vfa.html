<html>

<head>
<title>Lihong Li</title>
<meta name="author" content="Lihong Li">
<meta name="description" content="Personal Homepage">
<meta name="keywords" content="Artificial Intelligence, Machine Learning, Reinforcement Learning, Planning, Research, Rutgers, Yahoo!, Microsoft, Google">
<meta name="classification" content="Personal Homepage">
</head>

<body bgcolor="f7f7f7" text="black" link="blue" vlink="purple" alink="red">

<h1> <img src="../img/lihong-pro.jpg"> Lihong Li <img src="../img/chinese-name.jpg" height="25"></h1>

Research Scientist <br/>
Google Inc. <br/><br/>
lihongli.cs@gmail.com (for general academic work) <br/>
lihong@google.com (for Google related work) <br/>
747 Sixth Street South, Kirkland, WA, USA 98033<br/>
<p>

<p/><hr>

[<a href="../index.html#interests"><font face="Times New Roman" size="3">Interests</font></a><font face="Times New Roman" size="3">]
[<a href="../index.html#experiences"><font face="Times New Roman" size="3">Experiences</font></a><font face="Times New Roman" size="3">]
[</font><a href="../pubs.html" target="_self"><font face="Times New Roman" size="3">Publications</font></a><font face="Times New Roman" size="3">]
[<a href="../index.html#activities"><font face="Times New Roman" size="3">Professional Activities</font></a><font face="Times New Roman" size="3">]
[</font><a href="../index.html" target="_self"><font face="Times New Roman" size="3">Back to Home</font></a><font face="Times New Roman" size="3">]

<p/><hr>

<h2>Value Function Approximation (VFA) and Generalization</h2>

How can we estimate long-term reward of taking an action in a given state, using a compact representation?  This question lies in the heart of modern reinforcement learning, and is a key component in many prominent RL applications.  This problem is notoriously challenging (the ``deadly triad''): many of the classic algorithms are known to diverge, and it does happen very often in practice.  Our recent work (SBEED) provides the first provably convergent VFA algorithm in the controlled case, with nonlinear funnction classes, and with off-policy data.

<ul>
  
<li>Y. Feng, <b>L. Li</b>, and Q. Liu: A kernel loss for solving the Bellman equation.  In <i>Advances in Neural Information Processing Systems 32 (NeurIPS)</i>, 2019. [<a href="https://papers.nips.cc/paper/9679-a-kernel-loss-for-solving-the-bellman-equation">link</a>, <a href="http://arxiv.org/abs/1905.10506">arXiv</a>]

<li>B. Dai, A. Shaw, <b>L. Li</b>, L. Xiao, N. He, Z. Liu, J. Chen, and L. Song: SBEED: Convergent reinforcement learning with nonlinear function approximation.  In <i>the 35th International Conference on Machine Learning (ICML)</i>, 2018. [<a href="http://proceedings.mlr.press/v80/dai18c.html">link</a>, <a href="https://arxiv.org/abs/1712.10285">arXiv</a>, <a href="../slides/slides_dai18sbeed.pdf">slides</a>]

<li>Y. Chen, <b>L. Li</b>, and M. Wang: Bilinear Ï€ learning using state and action features.  In <i>the 35th International Conference on Machine Learning (ICML)</i>, 2018. [<a href="http://proceedings.mlr.press/v80/chen18e.html">link</a>, <a href="https://arxiv.org/abs/1804.10328">arXiv</a>]

<li>B. Dai, A. Shaw, N. He, <b>L. Li</b>, and L. Song: Boosting the actor with dual critic.  In <i>the 6th International Conference on Learning Representations (ICLR)</i>, 2018. [<a href="https://arxiv.org/abs/1712.10282">arXiv</a>]

<li>J. Chen, C. Wang, L. Xiao, J. He, <b>L. Li</b>, and L. Deng: Q-LDA: Uncovering latent patterns in text-based sequential decision processes.  In <i>Advances in Neural Information Processing Systems 30 (NIPS)</i>, 2017.  [<a href="http://papers.nips.cc/paper/7083-q-lda-uncovering-latent-patterns-in-text-based-sequential-decision-processes">link</a>]

<li>S. Du, J. Chen, <b>L. Li</b>, L. Xiao, and D. Zhou: Stochastic variance reduction methods for policy evaluation.  In <i>the 34th International Conference on Machine Learning (ICML)</i>, 2017. [<a href="http://proceedings.mlr.press/v70/du17a.html">link</a>]

<li><b>L. Li</b>: A worst-case comparison between temporal difference and residual gradient.  In <i>the 25th International Conference on Machine Learning (ICML)</i>, 2008.

<li>R. Parr, <b>L. Li</b>, G. Taylor, C. Painter-Wakefield, and M.L. Littman: An analysis of linear models, linear value function approximation, and feature selection for reinforcement learning. In <i>the 25th International Conference on Machine Learning (ICML)</i>, 2008.

<li><b>L. Li</b> and M.L. Littman: Efficient value-function approximation via online linear regression.  In <i>the 10th International Symposium on Artificial Intelligence and Mathematics (AI&Math)</i>, 2008.

<li>R. Parr, C. Painter-Wakefield, <b>L. Li</b>, and M.L. Littman: Analyzing feature generation for value-function approximation.  In <i>the 24th International Conference on Machine Learning (ICML)</i>, 2007.

<li><b>L. Li</b>, T.J. Walsh, and M.L. Littman: Towards a unified theory of state abstraction for MDPs. In <i>the 9th International Symposium on Artificial Intelligence and Mathematics (AI&Math)</i>, 2006.

<li><b>L. Li</b>, M.L. Littman: Lazy approximation for solving continuous finite-horizon MDPs. In <i>the 20th National Conference on Artificial Intelligence (AAAI)</i>, 2005.



<ul>

</ul>

</html>
