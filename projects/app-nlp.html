<html>

<script src="../header.js"></script>

[<a href="../index.html#interests"><font face="Times New Roman" size="3">Interests</font></a><font face="Times New Roman" size="3">]
[<a href="../index.html#experiences"><font face="Times New Roman" size="3">Experiences</font></a><font face="Times New Roman" size="3">]
[</font><a href="../pubs.html" target="_self"><font face="Times New Roman" size="3">Publications</font></a><font face="Times New Roman" size="3">]
[<a href="../index.html#activities"><font face="Times New Roman" size="3">Professional Activities</font></a><font face="Times New Roman" size="3">]
[</font><a href="../index.html" target="_self"><font face="Times New Roman" size="3">Back to Home</font></a><font face="Times New Roman" size="3">]

<p/><hr>

<h2>Applications to Natural Language Processing (NLP)</h2>

How can we introduce decision making abilities to natural language-based systems?  Some of the these NLP applications, such as conversational systems and more recently large language model post-training, can naturally be modeled in a decision-theoretic framework and optimized by reinforcement learning.

<ul>

<li>Z. Wei, W. Yao, Y. Liu, W. Zhang, Q. Lu, L. Qiu, C. Yu, P. Xu, C. Zhang, B. Yin, H. Yun, and <b>L. Li</b>: WebAgent-R1: Training web agents via end-to-end multi-turn reinforcement learning. In <i> the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, 2025.

<li>Q. Zhang, L. Qiu, I. Hong, Z. Xu, T. Liu, S. Li, R. Zhang, Z. Li, <b>L. Li</b>, B. Yin, C. Zhang, J. Chen, H. Jiang, and T. Zhao: Self-rewarding PPO: Aligning large language models with demonstrations only. In <i>the 2nd Conference on Language Modeling (COLM)</i>, 2025.

<li>J. Gao, M. Galley, and <b>L. Li</b>: Neural approaches to Conversational AI: Question answering, task-oriented dialogues and social chatbots.  Foundations and Trends in Information Retrieval, 13(2-3):127-298, 2019. ISBN 978-1-68083-552-6.  [<a href="https://www.nowpublishers.com/article/Details/INR-074">link</a>, <a href="https://arxiv.org/abs/1809.08267">arXiv</a>]

<li>D. Tang, X. Li, J. Gao, C. Wang, <b>L. Li</b>, and T. Jebara: Subgoal discovery for hierarchical dialogue policy learning.  In <i>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, 2018. [<a href="https://www.aclweb.org/anthology/D18-1253/">link</a>]

<li>Z. Lipton, X. Li, J. Gao, <b>L. Li</b>, F. Ahmed, and L. Deng: Efficient dialogue policy learning with BBQ-networks.  In <i>the 32nd AAAI Conference on Artificial Intelligence (AAAI)</i>, 2018. [<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16189">link</a>]

<li>J. Chen, C. Wang, L. Xiao, J. He, <b>L. Li</b>, and L. Deng: Q-LDA: Uncovering latent patterns in text-based sequential decision processes.  In <i>Advances in Neural Information Processing Systems 30 (NIPS)</i>, 2017.  [<a href="http://papers.nips.cc/paper/7083-q-lda-uncovering-latent-patterns-in-text-based-sequential-decision-processes">link</a>]

<li>B. Peng, X. Li, <b>L. Li</b>, J. Gao, A. Celikyilmaz, S. Lee, and K.-F. Wong: Composite task-completion dialogue system via hierarchical deep reinforcement learning.  In <i>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, 2017. [<a href="https://aclanthology.info/papers/D17-1237/d17-1237">link</a>]

<li>B. Dhingra, <b>L. Li</b>, X. Li, J. Gao, Y.-N. Chen, F. Ahmed, and L. Deng: Towards end-to-end reinforcement learning of dialogue agents for information access.  In <i>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</i>, 2017. [<a href="https://doi.org/10.18653/v1/P17-1045">link</a>]

<li>X. Li, Z.C. Lipton, B. Dhingra, <b>L. Li</b>, J. Gao, Y.-N. Chen: A user simulator for task-completion dialogues.  MSR technical report, December 2016.

<li>J. He, M. Ostendorf, X. He, J. Chen, J. Gao, <b>L. Li</b>, and L. Deng: Deep reinforcement learning with a combinatorial action space for predicting and tracking popular discussion threads.  In <i>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, 2016. [<a href="http://www.aclweb.org/anthology/D16-1189">link</a>]

<li>J. He, J. Chen, X. He, J. Gao, <b>L. Li</b>, L. Deng, and M. Ostendorf: Deep reinforcement learning with a natural language action space.  In <i>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</i>, 2016. [<a href="http://aclweb.org/anthology/P/P16/P16-1153.pdf">link</a>]

<li>J. He, J. Chen, X. He, J. Gao, <b>L. Li</b>, L. Deng, and M. Ostendorf: Deep reinforcement learning with an unbounded action space.  In <i>the International Conference on Learning Representations (ICLR), Workshop Track</i>, 2016.

<li><b>L. Li</b>, H. He, and J.D. Williams: Temporal supervised learning for inferring a dialog policy from example conversations.  In the <i>IEEE Spoken Language Technology Workshop (SLT)</i>, 2014.

<li><b>L. Li</b>, J.D. Williams, and S. Balakrishnan: Reinforcement learning for spoken dialog management using least-squares policy iteration and fast feature selection. In <i>the 10th Annual Conference of the International Speech Communication Association (INTERSPEECH)</i>, 2009.

</ul>

</html>
